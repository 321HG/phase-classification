{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import struct\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import sys #only needed to determine Python version number\n",
    "import matplotlib #only needed to determine Matplotlib version number\n",
    "from scipy import signal\n",
    "import h5py\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.models import load_model\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.externals import joblib\n",
    "import xgboost as xgb\n",
    "import gcforest.gcforest\n",
    "from gcforest.utils.config_utils import load_json\n",
    "from phase_utils import print_cm\n",
    "from phase_features_loader import PhaseFeaturesLoader\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "import autosklearn.classification\n",
    "from collections import Counter\n",
    "    \n",
    "# Enable inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "FEATURES_TINY = \"data/phase/ml_features_tiny.csv\"\n",
    "FEATURES = \"data/phase/ml_features.csv\"\n",
    "dataset_train = \"data/phase/ml_features_train.csv\"\n",
    "dataset_test = \"data/phase/ml_features_test.csv\"\n",
    "STA = \"URZ\"\n",
    "phases = [\"regP\", \"regS\", \"tele\", \"N\"]\n",
    "channels = [\"BHE\", \"BHZ\", \"BHN\"]\n",
    "validation_split = 0.1\n",
    "seed = 10\n",
    "file_stack1 = \"results/phase_stack_1.hdf5\"\n",
    "\n",
    "# parameters for NN:\n",
    "batch_size = 1024\n",
    "epochs = 500\n",
    "dropout = 0.25\n",
    "layers = [64, 64]\n",
    "phase_length = {\"URZ\": {\"regP\": 6840, \"regS\": 6840, \"tele\": 6840, \"N\": 6840*10}}\n",
    "model_file_path_nn = \"results/phase_nn.hdf5\"\n",
    "verbose = 0\n",
    "cross_validation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n",
      "Pandas version 0.22.0\n",
      "Matplotlib version 2.1.2\n"
     ]
    }
   ],
   "source": [
    "print('Python version ' + sys.version)\n",
    "print('Pandas version ' + pd.__version__)\n",
    "print('Matplotlib version ' + matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(model, x_test, y_test):\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(model.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = model.cv_results_['mean_test_score']\n",
    "    stds = model.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(x_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    column_width = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * column_width\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=\" \")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(column_width) % label, end=\" \")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(column_width) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(column_width) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacking():\n",
    "    def __init__(self, filename, classifiers, n_classes, data_length):\n",
    "        self.h5f = h5py.File(filename, \"w\")\n",
    "        try:\n",
    "            dset_classifier = self.h5f['/classifier']\n",
    "        except KeyError:\n",
    "            dset_classifier = self.h5f.create_dataset(\"classifier\", data = [c.encode() for c in classifiers])\n",
    "        probability = np.zeros(shape=(data_length, len(classifiers), n_classes))\n",
    "        try:\n",
    "            dset_probability = self.h5f['/probability']\n",
    "        except KeyError:\n",
    "            dset_probability = self.h5f.create_dataset(\"/probability\", data = probability)\n",
    "        try:\n",
    "            dset_y = self.h5f['/y']\n",
    "        except KeyError:\n",
    "            dset_y = self.h5f.create_dataset(\"/y\", data = np.zeros(data_length, dtype=\"int16\"))\n",
    "    \n",
    "    def save_prob(self, offset, classifier_index, probability):\n",
    "        dset_probability = self.h5f['/probability']\n",
    "        for i, prob in enumerate(probability):\n",
    "            dset_probability[offset+i, classifier_index, :] = prob\n",
    "            \n",
    "    def save_y(self, offset, y):\n",
    "        dset_y = self.h5f['/y']\n",
    "        dset_y[offset:offset+len(y)] = y\n",
    "    \n",
    "    def close(self):\n",
    "        self.h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify(y, n_classes=4):\n",
    "        'Returns labels in binary NumPy array'\n",
    "        return np.array([[1 if y[i] == j else 0 for j in range(n_classes)]\n",
    "                         for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(x, y, sampling_type=None):\n",
    "    if sampling_type == \"smoteenn\":\n",
    "        sme = SMOTEENN(random_state=1)\n",
    "        x_out, y_out = sme.fit_sample(x, y)\n",
    "    else:\n",
    "        if sampling_type == \"enn\":\n",
    "            enn = EditedNearestNeighbours(random_state=1)\n",
    "            x_out, y_out = enn.fit_sample(x, y)\n",
    "        else:\n",
    "            if sampling_type is None or sampling_type == \"nosampling\":\n",
    "                x_out, y_out = x, y\n",
    "        \n",
    "    print(\"Bevor reduction:\", sorted(Counter(y).items()))\n",
    "    print(\"After reduction:\", sorted(Counter(y_out).items()))\n",
    "    return x_out, y_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "ABC = ABCMeta('ABC', (object,), {})\n",
    "\n",
    "class Classifier(ABC):\n",
    "    __instances__ = dict()\n",
    "\n",
    "    def __init__(self):\n",
    "        Classifier.__instances__[self.__class__.__name__] = self\n",
    "    \n",
    "    def class_name(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    @staticmethod\n",
    "    def create_model():\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def fit(self, x_train, y_train, verbose=0, sampling_type=None):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, x_test, y_test=None, sampling_type=None):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, filename):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self, filename):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def resample(x, y, sampling_type=None):        \n",
    "        if sampling_type == \"smoteenn\":\n",
    "            sme = SMOTEENN(random_state=1)\n",
    "            x_out, y_out = sme.fit_sample(x, y)\n",
    "        else:\n",
    "            if sampling_type == \"enn\":\n",
    "                enn = EditedNearestNeighbours(random_state=1)\n",
    "                x_out, y_out = enn.fit_sample(x, y)\n",
    "            else:\n",
    "                if sampling_type is None or sampling_type == \"nosampling\":\n",
    "                    x_out, y_out = x, y\n",
    "        \n",
    "        print(\"Bevor reduction:\", sorted(Counter(y).items()))\n",
    "        print(\"After reduction:\", sorted(Counter(y_out).items()))\n",
    "        return x_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword argument repeated (<ipython-input-55-9b7b2749714a>, line 37)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-55-9b7b2749714a>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    input_height=input_height, input_width=input_width)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword argument repeated\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "class CNN(Classifier):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = None\n",
    "       \n",
    "    @staticmethod\n",
    "    def create_model(layers, dropout=0.1, n_features=16, input_height=4, input_width=4):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (3, 3), input_shape=(1, input_height, input_width), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Conv2D(256, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(256, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(4, activation='softmax'))\n",
    "        \n",
    "        #lrate = 0.01\n",
    "        #decay = lrate/epochs\n",
    "        #sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def fit(self, x_train, y_train, layers=[32, 64], verbose=0):\n",
    "        input_height = x_train.shape[1]\n",
    "        input_width = x_train.shape[2]\n",
    "        x_train = np.expand_dims(x_train, axis=1)\n",
    "        y_train = sparsify(y_train)\n",
    "        #y_train = np.expand_dims(y_train, axis=1)   \n",
    "        tensorboard = TensorBoard(log_dir='graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "        checkpoint = ModelCheckpoint(model_file_path_nn, monitor='acc', verbose=verbose,\n",
    "                                     save_best_only=True, mode='max')\n",
    "        self.model = CNN.create_model(layers=layers, dropout=dropout, input_height=5, \n",
    "                                     input_height=input_height, input_width=input_width)            \n",
    "        history = self.model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
    "                  validation_split=0.1, callbacks=[checkpoint, tensorboard])\n",
    "\n",
    "        print(\"Max of acc: {}, val_acc: {}\".\n",
    "              format(max(history.history[\"acc\"]), max(history.history[\"val_acc\"])))\n",
    "        print(\"Min of loss: {}, val_loss: {}\".\n",
    "              format(min(history.history[\"loss\"]), min(history.history[\"val_loss\"])))\n",
    "    \n",
    "    def predict(self, x_test, y_test=None):\n",
    "        x_test = np.expand_dims(x_test, axis=1) \n",
    "        if y_test is not None:\n",
    "            y_test = sparsify(y_test)\n",
    "            #y_test = np.expand_dims(y_test, axis=1)\n",
    "            score = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "            print(\"Accuracy: {}\".format(score[1]*100))\n",
    "        probability = self.model.predict(x_test, verbose=0)\n",
    "        return probability  \n",
    "        \n",
    "    def load(self, model_file_path):\n",
    "        self.model = load_model(model_file_path)\n",
    "        \n",
    "    def save(self, model_file_path):        \n",
    "        # save model to file\n",
    "        self.model.save(model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(Classifier):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = None\n",
    "        self.layers = [32, 32]\n",
    "       \n",
    "    @staticmethod\n",
    "    def create_model(layers, dropout=0.1, n_features=16):\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(layers[0], input_shape=(1, n_features), activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        for units in layers[1:]:\n",
    "            model.add(Dense(units, activation='relu'))\n",
    "            model.add(Dropout(dropout))\n",
    "        model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def set_layers(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def fit(self, x_train, y_train, verbose=0, sampling_type=None):\n",
    "        x_train, y_train = Classifier.resample(x_train, y_train, sampling_type)\n",
    "        x_train = np.expand_dims(x_train, axis=1)\n",
    "        y_train = sparsify(y_train)\n",
    "        y_train = np.expand_dims(y_train, axis=1)   \n",
    "        tensorboard = TensorBoard(log_dir='graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "        checkpoint = ModelCheckpoint(model_file_path_nn, monitor='acc', verbose=verbose,\n",
    "                                     save_best_only=True, mode='max')\n",
    "        self.model = NN.create_model(layers=self.layers, dropout=dropout)            \n",
    "        history = self.model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
    "                  validation_split=0.1, callbacks=[checkpoint, tensorboard])\n",
    "\n",
    "        print(\"Max of acc: {}, val_acc: {}\".\n",
    "              format(max(history.history[\"acc\"]), max(history.history[\"val_acc\"])))\n",
    "        print(\"Min of loss: {}, val_loss: {}\".\n",
    "              format(min(history.history[\"loss\"]), min(history.history[\"val_loss\"])))\n",
    "    \n",
    "    def predict(self, x_test, y_test=None, sampling_type=None):\n",
    "        x_test, y_test = Classifier.resample(x_test, y_test, sampling_type)\n",
    "        x_test = np.expand_dims(x_test, axis=1) \n",
    "        if y_test is not None:\n",
    "            y_test = sparsify(y_test)\n",
    "            y_test = np.expand_dims(y_test, axis=1)\n",
    "            score = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "            print(\"Accuracy: {}\".format(score[1]*100))\n",
    "        probability = self.model.predict(x_test, verbose=0)\n",
    "        return probability  \n",
    "        \n",
    "    def load(self, model_file_path):\n",
    "        self.model = load_model(model_file_path)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "    def save(self, model_file_path):        \n",
    "        # save model to file\n",
    "        self.model.save(model_file_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(Classifier):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = None\n",
    "     \n",
    "    @staticmethod\n",
    "    def create_model():\n",
    "        params_grid = [\n",
    "            #{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "            #{'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "            {'C': [1000], 'gamma': [0.001], 'kernel': ['rbf'], 'probability': [True]}\n",
    "        ]\n",
    "\n",
    "        model = GridSearchCV(svm.SVC(), params_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        return model\n",
    "    \n",
    "    def fit(self, x_train, y_train, verbose=0, sampling_type=None):\n",
    "        x_train, y_train = Classifier.resample(x_train, y_train, sampling_type)\n",
    "        self.model = SVM.create_model()\n",
    "        print(self.model)\n",
    "        self.model.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x_test, y_test=None, sampling_type=None):\n",
    "        x_test, y_test = Classifier.resample(x_test, y_test, sampling_type)\n",
    "        probability = self.model.predict_proba(x_test)\n",
    "        if y_test is not None:\n",
    "            y_pred = self.model.predict(x_test)\n",
    "            prediction = [np.round(value) for value in y_pred]\n",
    "            accuracy = accuracy_score(y_test, prediction)\n",
    "            print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "        return probability\n",
    "        \n",
    "    def load(self, model_file_path):\n",
    "        self.model = joblib.load(model_file_path)\n",
    "        \n",
    "    def save(self, model_file_path):     \n",
    "        # save model to file\n",
    "        joblib.dump(self.model, model_file_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoost(Classifier):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_model():\n",
    "        seed = 10\n",
    "        cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "        # set xgboost params\n",
    "        params_grid = {\n",
    "            'max_depth': [5, 6, 7, 8],\n",
    "            'n_estimators': [i for i in range(88, 92, 1)],\n",
    "            'learning_rate': np.linspace(0.1, 1, 20),\n",
    "            #'max_depth': [6],\n",
    "            #'n_estimators': [i for i in range(90, 91, 1)],\n",
    "            #'learning_rate': np.linspace(0.1, 1, 2),\n",
    "        }\n",
    "\n",
    "        params_fixed = {\n",
    "            'objective': 'multi:softprob',\n",
    "            'silent': 1,\n",
    "            'n_jobs': -1,\n",
    "            'verbose_eval': True\n",
    "        }\n",
    "\n",
    "        num_round = 30  # the number of training iterations\n",
    "\n",
    "        model = GridSearchCV(\n",
    "            estimator=xgb.XGBClassifier(**params_fixed, seed=seed),\n",
    "            param_grid=params_grid,\n",
    "            cv=cv,\n",
    "            scoring='accuracy'\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def fit(self, x_train, y_train, verbose=0, sampling_type=None):\n",
    "        x_train, y_train = Classifier.resample(x_train, y_train, sampling_type)\n",
    "        self.model = XGBoost.create_model()\n",
    "        print(self.model)\n",
    "        self.model.fit(x_train, y_train)\n",
    "        \n",
    "    def predict(self, x_test, y_test=None, sampling_type=None):\n",
    "        x_test, y_test = Classifier.resample(x_test, y_test, sampling_type)\n",
    "        probability = self.model.predict_proba(x_test)\n",
    "        print(y_test.shape)\n",
    "        y_list = np.zeros(4, dtype=int)\n",
    "        if y_test is not None:\n",
    "            for i in range(10):\n",
    "                print(y_test[len(y_test)-i-1], probability[len(y_test)-i-1])\n",
    "                print(x_test[len(y_test)-i-1])\n",
    "            for i in range(len(y_test)):\n",
    "                y_list[y_test[i]] += 1\n",
    "            print(y_list)\n",
    "            y_pred = self.model.predict(x_test)\n",
    "            prediction = [np.round(value) for value in y_pred]\n",
    "            # evaluate predictions\n",
    "            accuracy = accuracy_score(y_test, prediction)\n",
    "            print(\"Accuracy: {}\".format(accuracy * 100.0))\n",
    "        return probability\n",
    "        \n",
    "    def load(self, model_file_path):\n",
    "        self.model = joblib.load(model_file_path)\n",
    "        \n",
    "    def save(self, model_file_path):     \n",
    "        # save model to file\n",
    "        joblib.dump(self.model, model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCForest(Classifier):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = None\n",
    "     \n",
    "    @staticmethod\n",
    "    def create_model():\n",
    "        config = {\n",
    "              \"cascade\": {\n",
    "                  \"random_state\": 0,\n",
    "                  \"max_layers\": 100,\n",
    "                  \"early_stopping_rounds\": 3,\n",
    "                  \"n_classes\": 4,\n",
    "                  \"estimators\": [\n",
    "                      {\"n_folds\":5,\"type\":\"RandomForestClassifier\",\"n_estimators\":10,\"max_depth\":None,\"n_jobs\":-1},\n",
    "                      {\"n_folds\":5,\"type\":\"XGBClassifier\",\"n_estimators\":10,\"max_depth\":5,\n",
    "                           \"objective\":\"multi:softprob\", \"silent\":True, \"nthread\":-1, \n",
    "                           \"learning_rate\":0.1},\n",
    "                      {\"n_folds\":5,\"type\":\"ExtraTreesClassifier\",\"n_estimators\":10,\"max_depth\":None,\"n_jobs\":-1},\n",
    "                      {\"n_folds\":5,\"type\":\"LogisticRegression\"}\n",
    "                  ]\n",
    "              }\n",
    "            }\n",
    "\n",
    "        model = gcforest.gcforest.GCForest(config)\n",
    "        return model\n",
    "    \n",
    "    def fit(self, x_train, y_train, verbose=0, sampling_type=None):\n",
    "        x_train, y_train = Classifier.resample(x_train, y_train, sampling_type)\n",
    "        self.model = GCForest.create_model()\n",
    "        print(self.model)\n",
    "        self.model.fit_transform(x_train, y_train)\n",
    "        \n",
    "    def predict(self, x_test, y_test=None, sampling_type=None):\n",
    "        x_test, y_test = Classifier.resample(x_test, y_test, sampling_type)\n",
    "        probability = self.model.predict_proba(x_test)\n",
    "        if y_test is not None:\n",
    "            y_pred = self.model.predict(x_test)\n",
    "            prediction = [np.round(value) for value in y_pred]\n",
    "            accuracy = accuracy_score(y_test, prediction)\n",
    "            print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "        return probability\n",
    "        \n",
    "    def load(self, model_file_path):\n",
    "        self.model = joblib.load(model_file_path)\n",
    "        \n",
    "    def save(self, model_file_path):     \n",
    "        # save model to file\n",
    "        joblib.dump(self.model, model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoML(Classifier):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = None\n",
    "     \n",
    "    @staticmethod\n",
    "    def create_model():\n",
    "        model = autosklearn.classification.AutoSklearnClassifier()\n",
    "        return model\n",
    "    \n",
    "    def fit(self, x_train, y_train, verbose=0, sampling_type=None):\n",
    "        x_train, y_train = Classifier.resample(x_train, y_train, sampling_type)\n",
    "        self.model = AutoML.create_model()\n",
    "        print(self.model)\n",
    "        self.model.fit(x_train, y_train)\n",
    "        \n",
    "    def predict(self, x_test, y_test=None, sampling_type=None):\n",
    "        x_test, y_test = Classifier.resample(x_test, y_test, sampling_type)\n",
    "        probability = self.model.predict_proba(x_test)\n",
    "        if y_test is not None:\n",
    "            y_pred = self.model.predict(x_test)\n",
    "            prediction = [np.round(value) for value in y_pred]\n",
    "            accuracy = accuracy_score(y_test, prediction)\n",
    "            print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "        return probability\n",
    "        \n",
    "    def load(self, model_file_path):\n",
    "        self.model = joblib.load(model_file_path)\n",
    "        \n",
    "    def save(self, model_file_path):     \n",
    "        # save model to file\n",
    "        joblib.dump(self.model, model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML\n"
     ]
    }
   ],
   "source": [
    "aml=AutoML()\n",
    "print(aml.class_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length regP:6840\n",
      "length regS:6840\n",
      "length tele:6840\n",
      "length N:68400\n"
     ]
    }
   ],
   "source": [
    "# load train dataset\n",
    "pd_train = PhaseFeaturesLoader(filename=dataset_train, validation_split=validation_split,\n",
    "                         phase_length=phase_length, batch_size=batch_size)\n",
    "\n",
    "x_train, y_train = pd_train.get_dataset(expand_dim=False, y_onehot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length regP:2280\n",
      "length regS:2280\n",
      "length tele:2280\n",
      "length N:6840\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# load test dataset\n",
    "pd_test = PhaseFeaturesLoader(filename=dataset_test, phase_length=phase_length, batch_size=batch_size)\n",
    "x_test, y_test = pd_test.get_dataset(expand_dim=False, y_onehot=False)\n",
    "print(pd_test.get_phase_index(100089180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88920, 16)\n",
      "(13680, 16)\n",
      "[[ 2.24905878e-01  8.39121044e-01  2.51626980e-01 -4.57737841e-01\n",
      "  -2.44708634e-01 -4.58808887e-01 -1.60656316e-01 -1.12517864e-01\n",
      "  -2.37847791e-01 -8.44534544e-01  3.33333330e-01  7.24465430e-01\n",
      "   8.37405500e-01  1.00000000e-01  1.00000000e-01  2.20000000e+01]\n",
      " [ 5.98809844e-02  8.79091967e-01  1.85321354e-01 -1.70179184e+00\n",
      "  -9.99861612e-01  1.07490406e+00 -8.75815353e-01 -3.96305040e-01\n",
      "  -7.87249962e-01 -1.22670668e+00  3.33333330e-01  9.84692670e-01\n",
      "   9.86643400e-01  3.00000000e-01  3.96666670e-01  3.00000000e+00]\n",
      " [ 7.67638933e-01  6.75791133e-02  2.32719394e-01  2.03857427e-01\n",
      "   1.09118566e+00 -1.18806115e-01  6.12849642e-01  4.12443285e-02\n",
      "   8.72938755e-01 -6.11274756e-03  4.44444440e-01  7.24069300e-01\n",
      "   9.29103640e-01  1.00000000e-01  1.30000000e-01  1.90000000e+01]\n",
      " [ 2.35439667e-01  8.42885800e-01  4.27079100e-01 -1.06165944e+00\n",
      "  -1.33441403e+00  5.42280076e-01 -5.83749568e-01 -7.27541576e-01\n",
      "  -3.36668052e-01 -5.83395519e-01  1.00000000e+00  9.88398130e-01\n",
      "   9.92896820e-01  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      " [ 2.58779444e-01  9.80842000e-01  1.82520417e-01 -7.64042672e-01\n",
      "  -7.64042672e-01  3.45047272e-02 -1.98957783e-01 -1.75029429e-01\n",
      "  -7.35150999e-01 -1.01819155e+00  1.66666670e-01  9.22424440e-01\n",
      "   8.99833470e-01  3.00000000e-01  3.83333330e-01  1.10000000e+01]]\n",
      "[[ 1.14750656e-01  7.33134900e-01  2.70380573e-01 -1.50112984e+00\n",
      "  -8.82069872e-01 -7.47901234e-01 -2.12608565e-01 -9.12880813e-02\n",
      "  -5.02076002e-01 -9.90825917e-01  1.66666670e-01  9.85231160e-01\n",
      "   9.62273890e-01  1.00000000e-01  5.00000000e-01  1.20000000e+01]\n",
      " [ 7.66912978e-01  8.00411178e-01  7.69418804e-01  4.22975757e-01\n",
      "   9.31374417e-03 -1.54324447e-01 -1.02293099e-01  2.88217290e-01\n",
      "   8.13150766e-02 -1.15961063e-01  6.66666670e-01  9.60242370e-01\n",
      "   9.75366460e-01 -1.00000000e-01 -2.30000000e-01  1.90000000e+01]\n",
      " [ 8.04593644e-01  9.30780867e-01  6.23294144e-01  2.66260021e-01\n",
      "  -3.53762141e-01  1.04102461e-01 -6.01108332e-01  4.06909699e-02\n",
      "  -2.52864177e-01 -2.88735168e-01  4.44444440e-01  8.58538710e-01\n",
      "   7.64774030e-01  0.00000000e+00  0.00000000e+00  1.10000000e+01]\n",
      " [ 1.93497344e-01  9.68782089e-01  6.26250378e-01 -1.15837006e+00\n",
      "  -1.04689964e+00 -2.51079835e-01 -1.79984939e-01 -1.10838494e-01\n",
      "  -4.25336362e-01 -1.01697757e+00  1.66666670e-01  9.80293770e-01\n",
      "   9.50626710e-01  2.00000000e-01  3.65000000e-01  3.00000000e+00]\n",
      " [ 1.43634222e-01  8.75556356e-01  1.41723559e-01 -9.08303680e-01\n",
      "  -9.08303680e-01  2.32031819e-02 -1.20902973e-01  9.70416711e-02\n",
      "  -2.36781327e-01 -1.00480803e+00  1.66666670e-01  9.05172210e-01\n",
      "   8.79969470e-01  0.00000000e+00  0.00000000e+00  2.30000000e+01]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_train[0:5])\n",
    "print(x_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SVM': <class '__main__.SVM'>, 'NN': <class '__main__.NN'>, 'XGBoost': <class '__main__.XGBoost'>, 'AutoML': <class '__main__.AutoML'>, 'GCForest': <class '__main__.GCForest'>}\n",
      "{'SVM': 1, 'NN': 0, 'XGBoost': 2, 'AutoML': 4, 'GCForest': 3}\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\"NN\", \"SVM\", \"XGBoost\", \"GCForest\", \"AutoML\"]\n",
    "classifier_index = {classifier: i for i, classifier in enumerate(classifiers)}\n",
    "functions = globals().copy()\n",
    "classifier_class = {c: functions.get(c) for c in classifiers}\n",
    "print(classifier_class)\n",
    "print(classifier_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88920, 16)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_fit():\n",
    "    model = NN()\n",
    "    sampling_type=\"smoteenn\"\n",
    "    model.fit(x_train, y_train, verbose=0, sampling_type=sampling_type)\n",
    "    model.save(\"results/phase_train_{}_{}.mdl\".format(model.class_name().lower(), sampling_type))\n",
    "    model.predict(x_test, y_test, sampling_type=\"smoteenn\")\n",
    "\n",
    "run_model_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13680,)\n",
      "Bevor reduction: [(0, 2280), (1, 2280), (2, 2280), (3, 6840)]\n",
      "After reduction: [(0, 5852), (1, 5856), (2, 5372), (3, 3247)]\n",
      "Accuracy: 83.43090470861027\n"
     ]
    }
   ],
   "source": [
    "def run_model_predict():\n",
    "    model = NN()\n",
    "    model.load(\"results/phase_train_{}.mdl\".format(model.class_name().lower()))\n",
    "    print(y_test.shape)\n",
    "    model.predict(x_test, y_test, sampling_type=\"smoteenn\")\n",
    "\n",
    "run_model_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.SVM'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-a86c01c65775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrun_model_all_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-a86c01c65775>\u001b[0m in \u001b[0;36mrun_model_all_fit\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msampling_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"enn\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/phase_train_{}_{}.mdl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-8c2a10061fb2>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, verbose, sampling_type)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-67891ee785af>\u001b[0m in \u001b[0;36mresample\u001b[0;34m(x, y, sampling_type)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msampling_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"enn\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0menn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEditedNearestNeighbours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mx_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msampling_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msampling_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nosampling\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_sample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \"\"\"\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/imblearn/under_sampling/prototype_selection/edited_nearest_neighbours.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0my_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_class_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 nnhood_idx = self.nn_.kneighbors(\n\u001b[0;32m--> 196\u001b[0;31m                     X_class, return_distance=False)[:, 1:]\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mnnhood_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnnhood_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind_sel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mode'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 delayed(self._tree.query, check_pickle=False)(\n\u001b[1;32m    384\u001b[0m                     X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             )\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def run_model_all_fit():\n",
    "    for name in classifier_class:\n",
    "        print(classifier_class[name])\n",
    "        model = classifier_class[name]()\n",
    "        sampling_type=\"enn\"\n",
    "        model.fit(x_train, y_train, verbose=0, sampling_type=sampling_type)\n",
    "        model.save(\"results/phase_train_{}_{}.mdl\".format(model.class_name().lower(), sampling_type))\n",
    "        model.predict(x_test, y_test, sampling_type=sampling_type)\n",
    "\n",
    "run_model_all_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bevor reduction: [(0, 2280), (1, 2280), (2, 2280), (3, 6840)]\n",
      "After reduction: [(0, 2280), (1, 2280), (2, 2280), (3, 6840)]\n",
      "NN\n",
      "Bevor reduction: [(0, 2280), (1, 2280), (2, 2280), (3, 6840)]\n",
      "After reduction: [(0, 2280), (1, 2280), (2, 2280), (3, 6840)]\n",
      "Accuracy: 69.27631578947368\n",
      "SVM\n",
      "Bevor reduction: [(0, 2280), (1, 2280), (2, 2280), (3, 6840)]\n",
      "After reduction: [(0, 2280), (1, 2280), (2, 2280), (3, 6840)]\n",
      "Accuracy: 64.29%\n",
      "XGBoost\n",
      "Bevor reduction: [(0, 2280), (1, 2280), (2, 2280), (3, 6840)]\n",
      "After reduction: [(0, 2280), (1, 2280), (2, 2280), (3, 6840)]\n",
      "(13680,)\n",
      "1 [0.00289707 0.646213   0.01742636 0.33346358]\n",
      "[ 0.99100299  0.07024807  0.54329155  1.50837247  1.15566701 -0.69228997\n",
      "  0.19244289  0.53289275  1.11287722  0.51339931  0.33333333  0.94810108\n",
      "  0.9442341   0.1        -0.04        1.        ]\n",
      "3 [4.6110526e-03 1.2545275e-04 1.2481477e-04 9.9513865e-01]\n",
      "[ 0.99999908  0.85813236  3.73638315  8.37489032  8.37489032  4.92857758\n",
      "  5.48635213  6.51366594  7.86660195  8.4639867   0.16666667  1.\n",
      "  1.         -0.3         0.065       8.        ]\n",
      "1 [4.642022e-04 9.890386e-01 3.461418e-05 1.046264e-02]\n",
      "[ 9.44011567e-01  6.37963122e-02  3.14489334e-01  5.91445583e-01\n",
      "  5.65968271e-01  3.34046067e-01  4.51565456e-02 -1.68975000e-02\n",
      "  8.32557046e-01  3.57598958e-01  3.33333330e-01  7.47005590e-01\n",
      "  7.38786800e-01 -2.00000000e-01 -5.35000000e-01  2.20000000e+01]\n",
      "3 [1.5707109e-03 2.9691639e-05 3.4091322e-04 9.9805862e-01]\n",
      "[ 0.87207244  0.71356814  0.56841174  0.70848867 -0.51972839 -0.46921729\n",
      " -0.6103473   0.62331111  0.94541632  1.02652571  1.          0.90656611\n",
      "  0.93670484  0.          0.          8.        ]\n",
      "3 [4.4185263e-03 5.0428556e-04 9.1004789e-05 9.9498618e-01]\n",
      "[ 0.89207003  0.36023122  0.46824576  0.15729505  0.0976377  -0.51081554\n",
      "  0.24492968 -0.03327879 -0.09301036 -0.18438995  0.16666667  0.6891548\n",
      "  0.4575389   0.1         0.09       13.        ]\n",
      "0 [9.9383485e-01 4.7152276e-05 4.2042080e-03 1.9138141e-03]\n",
      "[ 0.18497378  0.96887469  0.17123307 -1.21637158 -1.25516106 -0.2966955\n",
      "  0.23606805 -0.28432212 -0.75878071 -1.35295871  0.16666667  0.98440685\n",
      "  0.98103284  0.1         0.36       16.        ]\n",
      "3 [1.2953506e-03 1.8306008e-04 3.7187649e-05 9.9848443e-01]\n",
      "[ 7.40914889e-01  7.41787567e-01  4.40172817e-01 -8.71891420e-03\n",
      "  2.07759496e-03 -6.45755125e-01 -5.08136468e-01  6.11747736e-02\n",
      " -5.65432417e-02 -1.02893219e-01  3.33333330e-01  7.03563500e-01\n",
      "  5.28045660e-01  0.00000000e+00  0.00000000e+00  2.30000000e+01]\n",
      "3 [9.4362698e-04 2.7127398e-05 2.9495417e-05 9.9899977e-01]\n",
      "[ 0.21423832  0.83658222  0.59213511 -0.74987068 -0.74987068 -0.51397656\n",
      " -0.66798647 -0.0837405  -0.03624489 -0.57543618  1.          0.88897736\n",
      "  0.95706077 -0.1        -0.25       16.        ]\n",
      "3 [1.4589898e-03 1.0651388e-03 6.1660292e-05 9.9741423e-01]\n",
      "[ 8.98316600e-01  2.98741644e-01  4.15290117e-01  5.93861646e-01\n",
      "  5.93861646e-01 -2.03310648e-03 -5.81455929e-03  1.84433503e-01\n",
      " -1.85138398e-01 -6.41019040e-02  1.66666670e-01  8.50085300e-01\n",
      "  8.14959050e-01 -1.00000000e-01 -1.90000000e-01  4.00000000e+00]\n",
      "3 [6.5276527e-04 6.2330550e-04 3.6812027e-05 9.9868709e-01]\n",
      "[ 0.89046963  0.40583043  0.38669238  0.34369301  0.20637886 -0.06145805\n",
      "  0.16262999  0.19250718  0.0364545  -0.07326984  0.16666667  0.78141307\n",
      "  0.74607038 -0.1        -0.06       21.        ]\n",
      "[2280 2280 2280 6840]\n",
      "Accuracy: 68.72807017543859\n",
      "GCForest\n",
      "Bevor reduction: [(0, 2280), (1, 2280), (2, 2280), (3, 6840)]\n",
      "After reduction: [(0, 2280), (1, 2280), (2, 2280), (3, 6840)]\n",
      "Accuracy: 69.25%\n",
      "AutoML\n",
      "Bevor reduction: [(0, 2280), (1, 2280), (2, 2280), (3, 6840)]\n",
      "After reduction: [(0, 2280), (1, 2280), (2, 2280), (3, 6840)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-55712abcf587>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstacking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mrun_layer1_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-55712abcf587>\u001b[0m in \u001b[0;36mrun_layer1_prediction\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/phase_train_{}_{}.mdl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mprobability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mstacking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstacking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-231d75d0f934>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x_test, y_test, sampling_type)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mprobability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/autosklearn/estimators.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, batch_size, n_jobs)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \"\"\"\n\u001b[1;32m    431\u001b[0m         return self._automl.predict_proba(\n\u001b[0;32m--> 432\u001b[0;31m             X, batch_size=batch_size, n_jobs=n_jobs)\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/autosklearn/automl.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, batch_size, n_jobs)\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;31m# return self._automl.predict(X, batch_size=batch_size, n_jobs=n_jobs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/autosklearn/automl.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, batch_size, n_jobs)\u001b[0m\n\u001b[1;32m    533\u001b[0m         all_predictions = joblib.Parallel(n_jobs=n_jobs)(\n\u001b[1;32m    534\u001b[0m             \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_model_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             for identifier in self.ensemble_.get_model_identifiers())\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_predictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/autosklearn/automl.py\u001b[0m in \u001b[0;36m_model_predict\u001b[0;34m(self, X, batch_size, identifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mX_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mX_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/autosklearn/pipeline/classification.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, batch_size)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \"\"\"\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_final_estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/autosklearn/pipeline/components/classification/__init__.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mestimator_supports_iterative_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/autosklearn/pipeline/components/classification/k_nearest_neighbors.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/neighbors/classification.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 delayed(self._tree.query, check_pickle=False)(\n\u001b[1;32m    384\u001b[0m                     X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             )\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def run_layer1_prediction():\n",
    "    sampling_type = \"enn\"\n",
    "    x_test_resampled, y_test_resampled = resample(x_test, y_test, sampling_type=\"nosampling\")\n",
    "    stacking_file = \"results/phase_test.hdf5\"\n",
    "    stacking = Stacking(stacking_file, classifiers, 4, len(x_test_resampled))\n",
    "    offset = 0\n",
    "    stacking.save_y(offset, y_test_resampled)\n",
    "    for index, classifier in enumerate(classifiers):\n",
    "        print(classifier)\n",
    "        model = classifier_class[classifier]()\n",
    "        model.load(\"results/phase_train_{}_{}.mdl\".format(classifier.lower(), sampling_type))\n",
    "        probability = model.predict(x_test_resampled, y_test_resampled, sampling_type=None)\n",
    "        stacking.save_prob(offset, index, probability)\n",
    "    stacking.close()\n",
    "    \n",
    "run_layer1_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bevor reduction: [(0, 6840), (1, 6840), (2, 6840), (3, 68400)]\n",
      "After reduction: [(0, 6840), (1, 411), (2, 779), (3, 52277)]\n",
      "(88920, 16) (60307, 16)\n",
      "Fold 0\n",
      "Training of NN\n",
      "Bevor reduction: [(0, 5466), (1, 332), (2, 631), (3, 41816)]\n",
      "After reduction: [(0, 5466), (1, 332), (2, 631), (3, 41816)]\n",
      "Max of acc: 0.958728696206153, val_acc: 1.0\n",
      "Min of loss: 0.11707239984463899, val_loss: 0.032452982666810556\n",
      "Bevor reduction: [(0, 1374), (1, 79), (2, 148), (3, 10461)]\n",
      "After reduction: [(0, 1374), (1, 79), (2, 148), (3, 10461)]\n",
      "Accuracy: 96.0122699386503\n",
      "Training of SVM\n",
      "Bevor reduction: [(0, 5466), (1, 332), (2, 631), (3, 41816)]\n",
      "After reduction: [(0, 5466), (1, 332), (2, 631), (3, 41816)]\n",
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False),\n",
      "       fit_params=None, iid=True, n_jobs=-1,\n",
      "       param_grid=[{'gamma': [0.001], 'kernel': ['rbf'], 'C': [1000], 'probability': [True]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring='accuracy', verbose=0)\n",
      "Bevor reduction: [(0, 1374), (1, 79), (2, 148), (3, 10461)]\n",
      "After reduction: [(0, 1374), (1, 79), (2, 148), (3, 10461)]\n",
      "Accuracy: 95.42%\n",
      "Training of XGBoost\n",
      "Bevor reduction: [(0, 5466), (1, 332), (2, 631), (3, 41816)]\n",
      "After reduction: [(0, 5466), (1, 332), (2, 631), (3, 41816)]\n",
      "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=10, shuffle=True),\n",
      "       error_score='raise',\n",
      "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=-1, nthread=None, objective='multi:softprob', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=10, silent=1,\n",
      "       subsample=1, verbose_eval=True),\n",
      "       fit_params=None, iid=True, n_jobs=1,\n",
      "       param_grid={'n_estimators': [88, 89, 90, 91], 'learning_rate': array([0.1    , 0.14737, 0.19474, 0.24211, 0.28947, 0.33684, 0.38421,\n",
      "       0.43158, 0.47895, 0.52632, 0.57368, 0.62105, 0.66842, 0.71579,\n",
      "       0.76316, 0.81053, 0.85789, 0.90526, 0.95263, 1.     ]), 'max_depth': [5, 6, 7, 8]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring='accuracy', verbose=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-10 16:24:39,140][cascade_classifier.fit_transform] X_groups_train.shape=[(48245, 16)],y_train.shape=(48245,),X_groups_test.shape=no_test,y_test.shape=no_test\n",
      "[ 2018-04-10 16:24:39,143][cascade_classifier.fit_transform] group_dims=[16]\n",
      "[ 2018-04-10 16:24:39,143][cascade_classifier.fit_transform] group_starts=[0]\n",
      "[ 2018-04-10 16:24:39,144][cascade_classifier.fit_transform] group_ends=[16]\n",
      "[ 2018-04-10 16:24:39,144][cascade_classifier.fit_transform] X_train.shape=(48245, 16),X_test.shape=(0, 16)\n",
      "[ 2018-04-10 16:24:39,147][cascade_classifier.fit_transform] [layer=0] look_indexs=[0], X_cur_train.shape=(48245, 16), X_cur_test.shape=(0, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bevor reduction: [(0, 1374), (1, 79), (2, 148), (3, 10461)]\n",
      "After reduction: [(0, 1374), (1, 79), (2, 148), (3, 10461)]\n",
      "(12062,)\n",
      "3 [2.0056302e-03 7.7784113e-03 4.4803430e-05 9.9017113e-01]\n",
      "[ 0.45329179  0.46690254  0.35767553 -0.20217528  0.32505494  0.50021904\n",
      "  0.12980171  0.04890782  0.15086452  0.33752555  0.44444444  0.77106119\n",
      "  0.83174534  0.          0.1        23.        ]\n",
      "3 [3.6125857e-04 3.9190785e-05 4.7291305e-06 9.9959487e-01]\n",
      "[ 0.81922789  0.29733404  0.64078448  0.60670014  0.70891086 -0.30428109\n",
      " -0.72216119 -0.13073366 -0.06405375 -0.23134637  1.          0.94612648\n",
      "  0.96313463  0.          0.         13.        ]\n",
      "3 [3.9348871e-04 5.8215005e-06 5.2200867e-06 9.9959546e-01]\n",
      "[ 0.27863799  0.7821791   0.4285989  -0.73011568 -0.4690811  -0.01051113\n",
      " -0.34130233 -0.23076488  0.11594554 -0.15242078  1.          0.92731593\n",
      "  0.99014002  0.          0.         10.        ]\n",
      "3 [4.5068486e-04 1.7706216e-05 4.3356572e-06 9.9952734e-01]\n",
      "[ 0.18659694  0.80907161  0.89047921 -0.92852952 -0.62371154  0.212936\n",
      " -0.62513014  0.20723527 -0.24613395 -0.09808702  1.          0.92770438\n",
      "  0.96970486 -0.1        -0.57        8.        ]\n",
      "3 [4.0117465e-04 3.5056839e-06 2.5516995e-06 9.9959272e-01]\n",
      "[ 0.5772982   0.9972639   0.68952569 -0.10448794 -0.4163062  -0.47992159\n",
      " -0.36585406 -0.42881227 -0.26290329 -0.03732057  1.          0.90701566\n",
      "  0.97416119 -0.1        -0.08        2.        ]\n",
      "3 [2.7109878e-03 5.3716787e-05 6.2575244e-05 9.9717271e-01]\n",
      "[ 0.18325593  0.93829789  0.24616025 -1.04118962 -0.85301641 -0.05331791\n",
      " -0.92651385 -0.21456128  0.5129379   0.23003331  1.          0.95304823\n",
      "  0.99406812  0.1         0.09        1.        ]\n",
      "3 [1.2101008e-03 9.8193595e-06 1.3422522e-05 9.9876666e-01]\n",
      "[ 0.12517794  0.92818433  0.23984937 -1.08929869 -0.87463629  0.20987546\n",
      " -0.83656817 -0.09201586 -0.07204906 -0.03572571  1.          0.94044752\n",
      "  0.98275569  0.          0.         20.        ]\n",
      "3 [3.9549472e-04 1.0543317e-05 8.8264096e-06 9.9958509e-01]\n",
      "[ 0.15282801  0.94544977  0.3354037  -1.13853042 -1.13853042 -0.23436908\n",
      " -1.14366938 -0.24148301 -0.22480893  0.61572843  1.          0.95766038\n",
      "  0.97185021  0.          0.         15.        ]\n",
      "3 [5.85226435e-03 6.64828331e-05 1.48036015e-05 9.94066477e-01]\n",
      "[ 0.43131729  0.76801158  0.30368886 -0.30711734 -0.07881705 -0.61627355\n",
      " -0.23235074 -0.26303765  0.02422731  0.00670764  0.16666667  0.78418879\n",
      "  0.35067383  0.          0.          0.        ]\n",
      "3 [1.4453419e-03 7.2481225e-06 9.0613457e-06 9.9853837e-01]\n",
      "[ 0.85780268  0.88650258  0.42614436  0.75794643 -0.75350853 -0.55798751\n",
      " -0.71353569  0.03995996 -0.11936876 -0.29863172  1.          0.92440889\n",
      "  0.97073479  0.1         0.44        0.        ]\n",
      "[ 1374    79   148 10461]\n",
      "Accuracy: 96.31072790581993\n",
      "Training of GCForest\n",
      "Bevor reduction: [(0, 5466), (1, 332), (2, 631), (3, 41816)]\n",
      "After reduction: [(0, 5466), (1, 332), (2, 631), (3, 41816)]\n",
      "<gcforest.gcforest.GCForest object at 0x7feb941d7550>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-10 16:24:39,582][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_0.predict)=95.50%\n",
      "[ 2018-04-10 16:24:40,006][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_1.predict)=95.82%\n",
      "[ 2018-04-10 16:24:40,426][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_2.predict)=95.66%\n",
      "[ 2018-04-10 16:24:40,848][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_3.predict)=95.62%\n",
      "[ 2018-04-10 16:24:41,271][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_4.predict)=95.82%\n",
      "[ 2018-04-10 16:24:41,273][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_cv.predict)=95.68%\n",
      "[ 2018-04-10 16:24:42,856][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_0.predict)=95.32%\n",
      "[ 2018-04-10 16:24:44,431][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_1.predict)=95.08%\n",
      "[ 2018-04-10 16:24:46,005][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_2.predict)=95.32%\n",
      "[ 2018-04-10 16:24:47,571][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_3.predict)=95.10%\n",
      "[ 2018-04-10 16:24:49,140][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_4.predict)=95.19%\n",
      "[ 2018-04-10 16:24:49,142][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_cv.predict)=95.20%\n",
      "[ 2018-04-10 16:24:49,367][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_0.predict)=95.57%\n",
      "[ 2018-04-10 16:24:49,590][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_1.predict)=95.65%\n",
      "[ 2018-04-10 16:24:49,810][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_2.predict)=95.81%\n",
      "[ 2018-04-10 16:24:50,032][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_3.predict)=95.83%\n",
      "[ 2018-04-10 16:24:50,253][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_4.predict)=95.94%\n",
      "[ 2018-04-10 16:24:50,255][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_cv.predict)=95.76%\n",
      "[ 2018-04-10 16:24:51,901][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_3 - 5_folds.train_0.predict)=94.96%\n",
      "[ 2018-04-10 16:24:53,515][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_3 - 5_folds.train_1.predict)=94.62%\n",
      "[ 2018-04-10 16:24:55,096][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_3 - 5_folds.train_2.predict)=94.64%\n",
      "[ 2018-04-10 16:24:56,731][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_3 - 5_folds.train_3.predict)=94.65%\n",
      "[ 2018-04-10 16:24:58,413][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_3 - 5_folds.train_4.predict)=94.51%\n",
      "[ 2018-04-10 16:24:58,415][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_3 - 5_folds.train_cv.predict)=94.68%\n",
      "[ 2018-04-10 16:24:58,417][cascade_classifier.calc_accuracy] Accuracy(layer_0 - train.classifier_average)=95.80%\n",
      "[ 2018-04-10 16:24:58,421][cascade_classifier.fit_transform] [layer=1] look_indexs=[0], X_cur_train.shape=(48245, 32), X_cur_test.shape=(0, 32)\n",
      "[ 2018-04-10 16:24:58,850][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_0.predict)=95.60%\n",
      "[ 2018-04-10 16:24:59,275][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_1.predict)=96.28%\n",
      "[ 2018-04-10 16:24:59,699][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_2.predict)=96.30%\n",
      "[ 2018-04-10 16:25:00,123][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_3.predict)=95.78%\n",
      "[ 2018-04-10 16:25:00,547][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_4.predict)=95.94%\n",
      "[ 2018-04-10 16:25:00,549][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_cv.predict)=95.98%\n",
      "[ 2018-04-10 16:25:02,997][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_0.predict)=95.86%\n",
      "[ 2018-04-10 16:25:05,436][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_1.predict)=96.41%\n",
      "[ 2018-04-10 16:25:07,878][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_2.predict)=96.03%\n",
      "[ 2018-04-10 16:25:10,311][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_3.predict)=96.13%\n",
      "[ 2018-04-10 16:25:12,752][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_4.predict)=96.37%\n",
      "[ 2018-04-10 16:25:12,754][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_cv.predict)=96.16%\n",
      "[ 2018-04-10 16:25:12,982][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_0.predict)=95.89%\n",
      "[ 2018-04-10 16:25:13,204][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_1.predict)=95.98%\n",
      "[ 2018-04-10 16:25:13,427][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_2.predict)=95.84%\n",
      "[ 2018-04-10 16:25:13,651][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_3.predict)=95.85%\n",
      "[ 2018-04-10 16:25:13,875][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_4.predict)=95.74%\n",
      "[ 2018-04-10 16:25:13,877][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_cv.predict)=95.86%\n",
      "[ 2018-04-10 16:25:16,047][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_3 - 5_folds.train_0.predict)=96.15%\n",
      "[ 2018-04-10 16:25:18,264][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_3 - 5_folds.train_1.predict)=96.11%\n",
      "[ 2018-04-10 16:25:20,441][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_3 - 5_folds.train_2.predict)=96.19%\n",
      "[ 2018-04-10 16:25:22,616][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_3 - 5_folds.train_3.predict)=96.30%\n",
      "[ 2018-04-10 16:25:24,925][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_3 - 5_folds.train_4.predict)=95.99%\n",
      "[ 2018-04-10 16:25:24,927][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_3 - 5_folds.train_cv.predict)=96.15%\n",
      "[ 2018-04-10 16:25:24,929][cascade_classifier.calc_accuracy] Accuracy(layer_1 - train.classifier_average)=96.22%\n",
      "[ 2018-04-10 16:25:24,933][cascade_classifier.fit_transform] [layer=2] look_indexs=[0], X_cur_train.shape=(48245, 32), X_cur_test.shape=(0, 32)\n",
      "[ 2018-04-10 16:25:25,366][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_0.predict)=95.98%\n",
      "[ 2018-04-10 16:25:25,791][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_1.predict)=95.71%\n",
      "[ 2018-04-10 16:25:26,214][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_2.predict)=96.28%\n",
      "[ 2018-04-10 16:25:26,638][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_3.predict)=96.03%\n",
      "[ 2018-04-10 16:25:27,063][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_4.predict)=95.96%\n",
      "[ 2018-04-10 16:25:27,065][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_cv.predict)=95.99%\n",
      "[ 2018-04-10 16:25:29,446][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_0.predict)=96.18%\n",
      "[ 2018-04-10 16:25:31,836][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_1.predict)=96.35%\n",
      "[ 2018-04-10 16:25:34,212][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_2.predict)=96.11%\n",
      "[ 2018-04-10 16:25:36,599][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_3.predict)=96.17%\n",
      "[ 2018-04-10 16:25:38,970][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_4.predict)=96.21%\n",
      "[ 2018-04-10 16:25:38,971][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_cv.predict)=96.20%\n",
      "[ 2018-04-10 16:25:39,200][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_0.predict)=95.92%\n",
      "[ 2018-04-10 16:25:39,424][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_1.predict)=95.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-10 16:25:39,647][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_2.predict)=95.95%\n",
      "[ 2018-04-10 16:25:39,870][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_3.predict)=95.94%\n",
      "[ 2018-04-10 16:25:40,093][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_4.predict)=95.68%\n",
      "[ 2018-04-10 16:25:40,095][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_cv.predict)=95.89%\n",
      "[ 2018-04-10 16:25:42,132][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_3 - 5_folds.train_0.predict)=96.27%\n",
      "[ 2018-04-10 16:25:44,153][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_3 - 5_folds.train_1.predict)=96.16%\n",
      "[ 2018-04-10 16:25:46,175][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_3 - 5_folds.train_2.predict)=96.21%\n",
      "[ 2018-04-10 16:25:48,071][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_3 - 5_folds.train_3.predict)=96.49%\n",
      "[ 2018-04-10 16:25:50,103][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_3 - 5_folds.train_4.predict)=95.93%\n",
      "[ 2018-04-10 16:25:50,105][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_3 - 5_folds.train_cv.predict)=96.21%\n",
      "[ 2018-04-10 16:25:50,107][cascade_classifier.calc_accuracy] Accuracy(layer_2 - train.classifier_average)=96.23%\n",
      "[ 2018-04-10 16:25:50,111][cascade_classifier.fit_transform] [layer=3] look_indexs=[0], X_cur_train.shape=(48245, 32), X_cur_test.shape=(0, 32)\n",
      "[ 2018-04-10 16:25:50,541][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 5_folds.train_0.predict)=96.00%\n",
      "[ 2018-04-10 16:25:50,966][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 5_folds.train_1.predict)=95.88%\n",
      "[ 2018-04-10 16:25:51,389][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 5_folds.train_2.predict)=95.95%\n",
      "[ 2018-04-10 16:25:51,813][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 5_folds.train_3.predict)=96.11%\n",
      "[ 2018-04-10 16:25:52,237][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 5_folds.train_4.predict)=95.75%\n",
      "[ 2018-04-10 16:25:52,239][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 5_folds.train_cv.predict)=95.94%\n",
      "[ 2018-04-10 16:25:54,628][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 5_folds.train_0.predict)=96.14%\n",
      "[ 2018-04-10 16:25:57,018][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 5_folds.train_1.predict)=96.28%\n",
      "[ 2018-04-10 16:25:59,397][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 5_folds.train_2.predict)=96.19%\n",
      "[ 2018-04-10 16:26:01,792][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 5_folds.train_3.predict)=96.22%\n",
      "[ 2018-04-10 16:26:04,154][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 5_folds.train_4.predict)=96.15%\n",
      "[ 2018-04-10 16:26:04,156][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 5_folds.train_cv.predict)=96.19%\n",
      "[ 2018-04-10 16:26:04,384][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 5_folds.train_0.predict)=96.01%\n",
      "[ 2018-04-10 16:26:04,607][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 5_folds.train_1.predict)=95.71%\n",
      "[ 2018-04-10 16:26:04,830][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 5_folds.train_2.predict)=96.03%\n",
      "[ 2018-04-10 16:26:05,053][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 5_folds.train_3.predict)=96.05%\n",
      "[ 2018-04-10 16:26:05,276][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 5_folds.train_4.predict)=95.86%\n",
      "[ 2018-04-10 16:26:05,278][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 5_folds.train_cv.predict)=95.93%\n",
      "[ 2018-04-10 16:26:07,402][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_3 - 5_folds.train_0.predict)=96.04%\n",
      "[ 2018-04-10 16:26:09,294][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_3 - 5_folds.train_1.predict)=96.27%\n",
      "[ 2018-04-10 16:26:11,252][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_3 - 5_folds.train_2.predict)=96.27%\n",
      "[ 2018-04-10 16:26:13,381][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_3 - 5_folds.train_3.predict)=96.43%\n",
      "[ 2018-04-10 16:26:15,439][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_3 - 5_folds.train_4.predict)=95.99%\n",
      "[ 2018-04-10 16:26:15,440][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_3 - 5_folds.train_cv.predict)=96.20%\n",
      "[ 2018-04-10 16:26:15,442][cascade_classifier.calc_accuracy] Accuracy(layer_3 - train.classifier_average)=96.22%\n",
      "[ 2018-04-10 16:26:15,445][cascade_classifier.fit_transform] [layer=4] look_indexs=[0], X_cur_train.shape=(48245, 32), X_cur_test.shape=(0, 32)\n",
      "[ 2018-04-10 16:26:15,876][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 5_folds.train_0.predict)=95.94%\n",
      "[ 2018-04-10 16:26:16,199][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 5_folds.train_1.predict)=95.99%\n",
      "[ 2018-04-10 16:26:16,623][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 5_folds.train_2.predict)=95.99%\n",
      "[ 2018-04-10 16:26:17,046][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 5_folds.train_3.predict)=95.80%\n",
      "[ 2018-04-10 16:26:17,469][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 5_folds.train_4.predict)=95.87%\n",
      "[ 2018-04-10 16:26:17,471][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 5_folds.train_cv.predict)=95.92%\n",
      "[ 2018-04-10 16:26:19,926][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 5_folds.train_0.predict)=96.32%\n",
      "[ 2018-04-10 16:26:22,329][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 5_folds.train_1.predict)=96.25%\n",
      "[ 2018-04-10 16:26:24,746][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 5_folds.train_2.predict)=96.25%\n",
      "[ 2018-04-10 16:26:27,102][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 5_folds.train_3.predict)=95.94%\n",
      "[ 2018-04-10 16:26:29,438][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 5_folds.train_4.predict)=96.10%\n",
      "[ 2018-04-10 16:26:29,439][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 5_folds.train_cv.predict)=96.17%\n",
      "[ 2018-04-10 16:26:29,667][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 5_folds.train_0.predict)=95.70%\n",
      "[ 2018-04-10 16:26:29,891][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 5_folds.train_1.predict)=96.07%\n",
      "[ 2018-04-10 16:26:30,114][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 5_folds.train_2.predict)=96.09%\n",
      "[ 2018-04-10 16:26:30,338][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 5_folds.train_3.predict)=95.86%\n",
      "[ 2018-04-10 16:26:30,562][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 5_folds.train_4.predict)=95.95%\n",
      "[ 2018-04-10 16:26:30,564][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 5_folds.train_cv.predict)=95.94%\n",
      "[ 2018-04-10 16:26:32,653][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_3 - 5_folds.train_0.predict)=96.45%\n",
      "[ 2018-04-10 16:26:34,513][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_3 - 5_folds.train_1.predict)=96.23%\n",
      "[ 2018-04-10 16:26:36,450][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_3 - 5_folds.train_2.predict)=96.08%\n",
      "[ 2018-04-10 16:26:38,406][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_3 - 5_folds.train_3.predict)=96.24%\n",
      "[ 2018-04-10 16:26:40,291][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_3 - 5_folds.train_4.predict)=95.89%\n",
      "[ 2018-04-10 16:26:40,293][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_3 - 5_folds.train_cv.predict)=96.18%\n",
      "[ 2018-04-10 16:26:40,295][cascade_classifier.calc_accuracy] Accuracy(layer_4 - train.classifier_average)=96.23%\n",
      "[ 2018-04-10 16:26:40,299][cascade_classifier.fit_transform] [layer=5] look_indexs=[0], X_cur_train.shape=(48245, 32), X_cur_test.shape=(0, 32)\n",
      "[ 2018-04-10 16:26:40,729][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 5_folds.train_0.predict)=96.10%\n",
      "[ 2018-04-10 16:26:41,155][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 5_folds.train_1.predict)=95.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-10 16:26:41,579][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 5_folds.train_2.predict)=95.70%\n",
      "[ 2018-04-10 16:26:41,902][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 5_folds.train_3.predict)=95.80%\n",
      "[ 2018-04-10 16:26:42,327][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 5_folds.train_4.predict)=96.03%\n",
      "[ 2018-04-10 16:26:42,329][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 5_folds.train_cv.predict)=95.89%\n",
      "[ 2018-04-10 16:26:44,776][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 5_folds.train_0.predict)=96.54%\n",
      "[ 2018-04-10 16:26:47,219][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 5_folds.train_1.predict)=95.93%\n",
      "[ 2018-04-10 16:26:49,600][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 5_folds.train_2.predict)=96.05%\n",
      "[ 2018-04-10 16:26:51,998][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 5_folds.train_3.predict)=95.97%\n",
      "[ 2018-04-10 16:26:54,401][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 5_folds.train_4.predict)=96.08%\n",
      "[ 2018-04-10 16:26:54,403][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 5_folds.train_cv.predict)=96.11%\n",
      "[ 2018-04-10 16:26:54,630][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 5_folds.train_0.predict)=96.26%\n",
      "[ 2018-04-10 16:26:54,851][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 5_folds.train_1.predict)=95.85%\n",
      "[ 2018-04-10 16:26:55,074][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 5_folds.train_2.predict)=95.70%\n",
      "[ 2018-04-10 16:26:55,297][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 5_folds.train_3.predict)=95.86%\n",
      "[ 2018-04-10 16:26:55,518][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 5_folds.train_4.predict)=95.58%\n",
      "[ 2018-04-10 16:26:55,521][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 5_folds.train_cv.predict)=95.85%\n",
      "[ 2018-04-10 16:26:57,520][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_3 - 5_folds.train_0.predict)=95.80%\n",
      "[ 2018-04-10 16:26:59,419][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_3 - 5_folds.train_1.predict)=96.64%\n",
      "[ 2018-04-10 16:27:01,369][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_3 - 5_folds.train_2.predict)=96.38%\n",
      "[ 2018-04-10 16:27:03,234][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_3 - 5_folds.train_3.predict)=96.23%\n",
      "[ 2018-04-10 16:27:05,021][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_3 - 5_folds.train_4.predict)=95.84%\n",
      "[ 2018-04-10 16:27:05,023][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_3 - 5_folds.train_cv.predict)=96.18%\n",
      "[ 2018-04-10 16:27:05,024][cascade_classifier.calc_accuracy] Accuracy(layer_5 - train.classifier_average)=96.18%\n",
      "[ 2018-04-10 16:27:05,025][cascade_classifier.fit_transform] [Result][Optimal Level Detected] opt_layer_num=3, accuracy_train=96.23%, accuracy_test=0.00%\n",
      "[ 2018-04-10 16:27:05,376][cascade_classifier.transform] X_groups_test.shape=[(12062, 16)]\n",
      "[ 2018-04-10 16:27:05,377][cascade_classifier.transform] group_dims=[16]\n",
      "[ 2018-04-10 16:27:05,378][cascade_classifier.transform] X_test.shape=(12062, 16)\n",
      "[ 2018-04-10 16:27:05,379][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(12062, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bevor reduction: [(0, 1374), (1, 79), (2, 148), (3, 10461)]\n",
      "After reduction: [(0, 1374), (1, 79), (2, 148), (3, 10461)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-10 16:27:06,492][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(12062, 32)\n",
      "[ 2018-04-10 16:27:07,611][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(12062, 32)\n",
      "[ 2018-04-10 16:27:08,727][cascade_classifier.transform] X_groups_test.shape=[(12062, 16)]\n",
      "[ 2018-04-10 16:27:08,728][cascade_classifier.transform] group_dims=[16]\n",
      "[ 2018-04-10 16:27:08,728][cascade_classifier.transform] X_test.shape=(12062, 16)\n",
      "[ 2018-04-10 16:27:08,729][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(12062, 16)\n",
      "[ 2018-04-10 16:27:09,840][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(12062, 32)\n",
      "[ 2018-04-10 16:27:10,958][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(12062, 32)\n",
      "[ 2018-04-10 16:27:12,094][logging_.warning] Could not delete output dir: /tmp/autosklearn_output_21875_2383\n",
      "[ 2018-04-10 16:27:12,094][logging_.warning] Could not delete tmp dir: /tmp/autosklearn_tmp_21875_2383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.34%\n",
      "Training of AutoML\n",
      "Bevor reduction: [(0, 5466), (1, 332), (2, 631), (3, 41816)]\n",
      "After reduction: [(0, 5466), (1, 332), (2, 631), (3, 41816)]\n",
      "AutoSklearnClassifier(delete_output_folder_after_terminate=True,\n",
      "           delete_tmp_folder_after_terminate=True,\n",
      "           disable_evaluator_output=False, ensemble_nbest=50,\n",
      "           ensemble_size=50, exclude_estimators=None,\n",
      "           exclude_preprocessors=None, get_smac_object_callback=None,\n",
      "           include_estimators=None, include_preprocessors=None,\n",
      "           initial_configurations_via_metalearning=25,\n",
      "           ml_memory_limit=3072, output_folder=None,\n",
      "           per_run_time_limit=360, resampling_strategy='holdout',\n",
      "           resampling_strategy_arguments=None, seed=1, shared_mode=False,\n",
      "           smac_scenario_args=None, time_left_for_this_task=3600,\n",
      "           tmp_folder=None)\n",
      "[WARNING] [2018-04-10 16:29:41,698:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-04-10 16:29:41,698:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "Bevor reduction: [(0, 1374), (1, 79), (2, 148), (3, 10461)]\n",
      "After reduction: [(0, 1374), (1, 79), (2, 148), (3, 10461)]\n",
      "Accuracy: 96.40%\n",
      "Fold 1\n",
      "Training of NN\n",
      "Bevor reduction: [(0, 5500), (1, 333), (2, 605), (3, 41807)]\n",
      "After reduction: [(0, 5500), (1, 333), (2, 605), (3, 41807)]\n",
      "Max of acc: 0.9577383693930522, val_acc: 1.0\n",
      "Min of loss: 0.12035879232487004, val_loss: 0.028538748010452548\n",
      "Bevor reduction: [(0, 1340), (1, 78), (2, 174), (3, 10470)]\n",
      "After reduction: [(0, 1340), (1, 78), (2, 174), (3, 10470)]\n",
      "Accuracy: 96.36876139943624\n",
      "Training of SVM\n",
      "Bevor reduction: [(0, 5500), (1, 333), (2, 605), (3, 41807)]\n",
      "After reduction: [(0, 5500), (1, 333), (2, 605), (3, 41807)]\n",
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False),\n",
      "       fit_params=None, iid=True, n_jobs=-1,\n",
      "       param_grid=[{'gamma': [0.001], 'kernel': ['rbf'], 'C': [1000], 'probability': [True]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring='accuracy', verbose=0)\n",
      "Bevor reduction: [(0, 1340), (1, 78), (2, 174), (3, 10470)]\n",
      "After reduction: [(0, 1340), (1, 78), (2, 174), (3, 10470)]\n",
      "Accuracy: 95.65%\n",
      "Training of XGBoost\n",
      "Bevor reduction: [(0, 5500), (1, 333), (2, 605), (3, 41807)]\n",
      "After reduction: [(0, 5500), (1, 333), (2, 605), (3, 41807)]\n",
      "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=10, shuffle=True),\n",
      "       error_score='raise',\n",
      "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=-1, nthread=None, objective='multi:softprob', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=10, silent=1,\n",
      "       subsample=1, verbose_eval=True),\n",
      "       fit_params=None, iid=True, n_jobs=1,\n",
      "       param_grid={'n_estimators': [88, 89, 90, 91], 'learning_rate': array([0.1    , 0.14737, 0.19474, 0.24211, 0.28947, 0.33684, 0.38421,\n",
      "       0.43158, 0.47895, 0.52632, 0.57368, 0.62105, 0.66842, 0.71579,\n",
      "       0.76316, 0.81053, 0.85789, 0.90526, 0.95263, 1.     ]), 'max_depth': [5, 6, 7, 8]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring='accuracy', verbose=0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-0a6656528218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msplit_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mrun_stacking_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-0a6656528218>\u001b[0m in \u001b[0;36mrun_stacking_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training of\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/phase_train_train_{}_{}.mdl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mprobability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-b7abc2e2bdcb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, verbose, sampling_type)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBoost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model)\u001b[0m\n\u001b[1;32m    504\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m                               verbose_eval=verbose, xgb_model=None)\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 898\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def run_stacking_train():\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    split_counter = 0\n",
    "    sampling_type=\"enn\"\n",
    "    x_train_resampled, y_train_resampled = resample(x_train, y_train, sampling_type=sampling_type)\n",
    "    print(x_train.shape, x_train_resampled.shape)\n",
    "    for train_index, test_index in kf.split(x_train_resampled):\n",
    "        print(\"Fold\", split_counter)\n",
    "        x_train_train = x_train_resampled[train_index]\n",
    "        y_train_train = y_train_resampled[train_index]\n",
    "        x_train_test = x_train_resampled[test_index]\n",
    "        y_train_test = y_train_resampled[test_index]\n",
    "        for classifier in classifiers:\n",
    "            print(\"Training of\", classifier)\n",
    "            model = classifier_class[classifier]()\n",
    "            model.fit(x_train_train, y_train_train)\n",
    "            model.save(\"results/phase_train_train_{}_{}.mdl\".format(classifier.lower(), split_counter))\n",
    "            probability = model.predict(x_train_test, y_train_test)\n",
    "            #print(\"results/phase_train_train_{}_{}.mdl\".format(classifier.lower(), split_counter))\n",
    "        split_counter += 1\n",
    "    \n",
    "run_stacking_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88920, 16)\n",
      "Bevor reduction: [(0, 6840), (1, 6840), (2, 6840), (3, 68400)]\n",
      "After reduction: [(0, 6840), (1, 411), (2, 779), (3, 52277)]\n",
      "(88920, 16) (60307, 16)\n",
      "Fold 0\n",
      "Bevor reduction: [(0, 1371), (1, 77), (2, 145), (3, 10469)]\n",
      "After reduction: [(0, 1371), (1, 77), (2, 145), (3, 10469)]\n",
      "Accuracy: 96.38534239860064\n",
      "Bevor reduction: [(0, 1371), (1, 77), (2, 145), (3, 10469)]\n",
      "After reduction: [(0, 1371), (1, 77), (2, 145), (3, 10469)]\n",
      "Accuracy: 95.66%\n",
      "Bevor reduction: [(0, 1371), (1, 77), (2, 145), (3, 10469)]\n",
      "After reduction: [(0, 1371), (1, 77), (2, 145), (3, 10469)]\n",
      "(12062,)\n",
      "3 [2.0056302e-03 7.7784113e-03 4.4803430e-05 9.9017113e-01]\n",
      "[ 0.45329179  0.46690254  0.35767553 -0.20217528  0.32505494  0.50021904\n",
      "  0.12980171  0.04890782  0.15086452  0.33752555  0.44444444  0.77106119\n",
      "  0.83174534  0.          0.1        23.        ]\n",
      "3 [2.4783371e-03 8.0803479e-04 1.4124614e-05 9.9669951e-01]\n",
      "[ 0.77894914  0.7317228   0.61671509  0.22924141  0.13544126 -0.31145177\n",
      " -0.98353675 -0.15852907  0.39709655  0.55065395  0.16666667  0.85516954\n",
      "  0.83601195  0.          0.          9.        ]\n",
      "3 [7.8113179e-04 7.1366267e-05 1.0804494e-05 9.9913675e-01]\n",
      "[ 0.83841579  0.6182509   0.40651547  0.37798995  0.12528063  0.57060885\n",
      " -0.36549605 -0.0434176   0.15059761 -0.18050178  0.66666667  0.79965954\n",
      "  0.98311418  0.          0.          3.        ]\n",
      "3 [3.9348871e-04 5.8215005e-06 5.2200867e-06 9.9959546e-01]\n",
      "[ 0.27863799  0.7821791   0.4285989  -0.73011568 -0.4690811  -0.01051113\n",
      " -0.34130233 -0.23076488  0.11594554 -0.15242078  1.          0.92731593\n",
      "  0.99014002  0.          0.         10.        ]\n",
      "3 [3.7014552e-03 3.4773013e-05 9.4745361e-04 9.9531639e-01]\n",
      "[ 0.04980493  0.99549624  0.39934438 -1.78039672 -1.78039672 -0.67054688\n",
      " -0.03439746 -0.95327483  0.29621342 -0.64865416  1.          0.98650437\n",
      "  0.99098427  0.          0.         14.        ]\n",
      "3 [1.2101008e-03 9.8193595e-06 1.3422522e-05 9.9876666e-01]\n",
      "[ 0.12517794  0.92818433  0.23984937 -1.08929869 -0.87463629  0.20987546\n",
      " -0.83656817 -0.09201586 -0.07204906 -0.03572571  1.          0.94044752\n",
      "  0.98275569  0.          0.         20.        ]\n",
      "3 [7.4864185e-04 1.1724350e-02 1.1705998e-05 9.8751533e-01]\n",
      "[ 8.08443589e-01  4.47070856e-01  3.50564608e-01  3.89248770e-01\n",
      "  2.39213162e-01  6.99636343e-01  7.48830919e-01  4.79329429e-03\n",
      "  2.93658724e-01 -6.77088042e-02  1.66666670e-01  8.34152160e-01\n",
      "  8.50714290e-01 -1.00000000e-01 -1.00000000e-01  1.00000000e+01]\n",
      "3 [1.4453419e-03 7.2481225e-06 9.0613457e-06 9.9853837e-01]\n",
      "[ 0.85780268  0.88650258  0.42614436  0.75794643 -0.75350853 -0.55798751\n",
      " -0.71353569  0.03995996 -0.11936876 -0.29863172  1.          0.92440889\n",
      "  0.97073479  0.1         0.44        0.        ]\n",
      "3 [2.2657785e-01 6.2669111e-05 2.1428989e-02 7.5193048e-01]\n",
      "[ 1.34046744e-02  9.87378956e-01  3.56897913e-01 -1.77701934e+00\n",
      " -1.77701934e+00 -5.84517156e-01 -5.40687185e-01 -1.20444900e-01\n",
      " -2.57580089e-01 -1.40400384e+00  3.33333330e-01  9.83517080e-01\n",
      "  9.88853580e-01  0.00000000e+00  0.00000000e+00  1.70000000e+01]\n",
      "3 [2.8041990e-03 5.1121388e-06 1.6592532e-05 9.9717402e-01]\n",
      "[ 0.74316732  0.9822943   0.81210678  0.41791506 -0.35968481 -0.14097891\n",
      " -0.20003056 -0.50148397 -0.37981975 -0.51507449  1.          0.98064686\n",
      "  0.90920513  0.          0.         20.        ]\n",
      "[ 1371    77   145 10469]\n",
      "Accuracy: 98.89736362129\n",
      "Bevor reduction: [(0, 1371), (1, 77), (2, 145), (3, 10469)]\n",
      "After reduction: [(0, 1371), (1, 77), (2, 145), (3, 10469)]\n",
      "Accuracy: 99.10%\n",
      "Bevor reduction: [(0, 1371), (1, 77), (2, 145), (3, 10469)]\n",
      "After reduction: [(0, 1371), (1, 77), (2, 145), (3, 10469)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-569b700afb5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mstacking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mrun_stacking_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-569b700afb5d>\u001b[0m in \u001b[0;36mrun_stacking_save\u001b[0;34m(x_train, y_train)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/phase_train_train_{}_{}.mdl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mprobability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mstacking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moffset\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-231d75d0f934>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x_test, y_test, sampling_type)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mprobability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/autosklearn/estimators.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, batch_size, n_jobs)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \"\"\"\n\u001b[1;32m    431\u001b[0m         return self._automl.predict_proba(\n\u001b[0;32m--> 432\u001b[0;31m             X, batch_size=batch_size, n_jobs=n_jobs)\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/autosklearn/automl.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, batch_size, n_jobs)\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;31m# return self._automl.predict(X, batch_size=batch_size, n_jobs=n_jobs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/autosklearn/automl.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, batch_size, n_jobs)\u001b[0m\n\u001b[1;32m    533\u001b[0m         all_predictions = joblib.Parallel(n_jobs=n_jobs)(\n\u001b[1;32m    534\u001b[0m             \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_model_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             for identifier in self.ensemble_.get_model_identifiers())\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_predictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/autosklearn/automl.py\u001b[0m in \u001b[0;36m_model_predict\u001b[0;34m(self, X, batch_size, identifier)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mX_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mX_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/autosklearn/pipeline/classification.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, batch_size)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \"\"\"\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_final_estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/autosklearn/pipeline/components/classification/__init__.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mestimator_supports_iterative_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/autosklearn/pipeline/components/classification/k_nearest_neighbors.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/neighbors/classification.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 delayed(self._tree.query, check_pickle=False)(\n\u001b[1;32m    384\u001b[0m                     X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             )\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/phase/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "def run_stacking_save(x_train, y_train):\n",
    "    print(x_train.shape)\n",
    "    stacking_file = \"results/phase_train_train.hdf5\"\n",
    "    stacking = Stacking(stacking_file, classifiers, 4, len(x_train))\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    split_counter = 0\n",
    "    offset = 0\n",
    "    sampling_type=\"enn\"\n",
    "    x_train_resampled, y_train_resampled = resample(x_train, y_train, sampling_type=sampling_type)\n",
    "    print(x_train.shape, x_train_resampled.shape)\n",
    "    for train_index, test_index in kf.split(x_train_resampled):\n",
    "        print(\"Fold\", split_counter)\n",
    "        x_train_train = x_train_resampled[train_index]\n",
    "        y_train_train = y_train_resampled[train_index]\n",
    "        x_train_test = x_train_resampled[test_index]\n",
    "        y_train_test = y_train_resampled[test_index]\n",
    "        stacking.save_y(offset, y_train_test)\n",
    "        for index, classifier in enumerate(classifiers):\n",
    "            model = classifier_class[classifier]()\n",
    "            model.load(\"results/phase_train_train_{}_{}.mdl\".format(classifier.lower(), split_counter))\n",
    "            probability = model.predict(x_train_test, y_train_test)\n",
    "            stacking.save_prob(offset, index, probability)\n",
    "        offset += len(y_train_test)\n",
    "        split_counter += 1\n",
    "    stacking.close()\n",
    "    \n",
    "run_stacking_save(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[9.97790456e-01 3.21820172e-32 2.20952998e-03 4.91114562e-08]\n",
      "  [9.53856562e-01 3.15069537e-05 4.59801149e-02 1.31816064e-04]\n",
      "  [9.99387860e-01 8.13680890e-06 4.58136608e-04 1.45875223e-04]\n",
      "  [9.06184733e-01 2.79739648e-02 3.31049338e-02 3.27363797e-02]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      " [[9.95055676e-01 1.26542756e-25 4.93491767e-03 9.45326519e-06]\n",
      "  [9.70011668e-01 2.42403309e-04 2.76831996e-02 2.06272875e-03]\n",
      "  [9.99254763e-01 7.94698190e-06 4.17023723e-04 3.20245192e-04]\n",
      "  [9.05653298e-01 2.81162709e-02 3.23107913e-02 3.39196436e-02]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      " [[9.73188221e-01 9.08296535e-36 2.68082228e-02 3.54328949e-06]\n",
      "  [9.50109935e-01 3.13789410e-05 4.95802368e-02 2.78448818e-04]\n",
      "  [9.90337253e-01 2.06483801e-05 9.31871869e-03 3.23410815e-04]\n",
      "  [8.97602081e-01 2.81416588e-02 4.24370393e-02 3.18192579e-02]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]]\n",
      "(88920, 5, 4)\n",
      "[0 0 0]\n",
      "(88920,)\n",
      "[78229, 77, 145, 10469]\n"
     ]
    }
   ],
   "source": [
    "stacking_file = \"results/phase_train_train.hdf5\"\n",
    "h5f = h5py.File(stacking_file, \"r\")\n",
    "dset_probability = h5f['/probability']\n",
    "print(dset_probability[0:3,:,:])\n",
    "print(dset_probability.shape)\n",
    "dset_y = h5f['/y']\n",
    "print(dset_y[0:3])\n",
    "print(dset_y.shape)\n",
    "y_counter = [0,0,0,0]\n",
    "for i, y in enumerate(dset_y):\n",
    "    y_counter[int(y)] += 1\n",
    "print(y_counter)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88920, 5, 4)\n",
      "(88920, 5, 4)\n",
      "Train on 80028 samples, validate on 8892 samples\n",
      "Epoch 1/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9453\n",
      "Epoch 00001: acc improved from -inf to 0.94538, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 2s 28us/step - loss: 0.2215 - acc: 0.9454 - val_loss: 3.3379e-06 - val_acc: 1.0000\n",
      "Epoch 2/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0101 - acc: 0.9969\n",
      "Epoch 00002: acc improved from 0.94538 to 0.99703, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 3/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0080 - acc: 0.9977\n",
      "Epoch 00003: acc improved from 0.99703 to 0.99775, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0083 - acc: 0.9978 - val_loss: 1.4305e-06 - val_acc: 1.0000\n",
      "Epoch 4/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0068 - acc: 0.9986\n",
      "Epoch 00004: acc improved from 0.99775 to 0.99860, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0068 - acc: 0.9986 - val_loss: 8.9407e-07 - val_acc: 1.0000\n",
      "Epoch 5/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0060 - acc: 0.9989\n",
      "Epoch 00005: acc improved from 0.99860 to 0.99888, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0058 - acc: 0.9989 - val_loss: 2.9802e-07 - val_acc: 1.0000\n",
      "Epoch 6/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0057 - acc: 0.9990\n",
      "Epoch 00006: acc improved from 0.99888 to 0.99894, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0058 - acc: 0.9989 - val_loss: 2.3842e-07 - val_acc: 1.0000\n",
      "Epoch 7/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0051 - acc: 0.9991\n",
      "Epoch 00007: acc improved from 0.99894 to 0.99905, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0052 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 8/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0050 - acc: 0.9990\n",
      "Epoch 00008: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0050 - acc: 0.9990 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 9/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0050 - acc: 0.9990\n",
      "Epoch 00009: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0049 - acc: 0.9990 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 10/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0049 - acc: 0.9991\n",
      "Epoch 00010: acc improved from 0.99905 to 0.99909, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0049 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 11/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0051 - acc: 0.9991\n",
      "Epoch 00011: acc improved from 0.99909 to 0.99911, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0051 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 12/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0051 - acc: 0.9991\n",
      "Epoch 00012: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0051 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 13/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0048 - acc: 0.9991\n",
      "Epoch 00013: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0047 - acc: 0.9990 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 14/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0046 - acc: 0.9991\n",
      "Epoch 00014: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0047 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 15/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0046 - acc: 0.9991\n",
      "Epoch 00015: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0046 - acc: 0.9990 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 16/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0049 - acc: 0.9990\n",
      "Epoch 00016: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0047 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 17/500\n",
      "78848/80028 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9991\n",
      "Epoch 00017: acc improved from 0.99911 to 0.99913, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0045 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 18/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0049 - acc: 0.9991\n",
      "Epoch 00018: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0048 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 19/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9991\n",
      "Epoch 00019: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0045 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 20/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9991\n",
      "Epoch 00020: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0044 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 21/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0045 - acc: 0.9991\n",
      "Epoch 00021: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0046 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 22/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0046 - acc: 0.9991\n",
      "Epoch 00022: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0045 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 23/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0045 - acc: 0.9991\n",
      "Epoch 00023: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0045 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 24/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0047 - acc: 0.9991\n",
      "Epoch 00024: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0045 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 25/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0043 - acc: 0.9991\n",
      "Epoch 00025: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0046 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 26/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9991\n",
      "Epoch 00026: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0044 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 27/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9991\n",
      "Epoch 00027: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0046 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 28/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0046 - acc: 0.9991\n",
      "Epoch 00028: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0046 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 29/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0047 - acc: 0.9991\n",
      "Epoch 00029: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0047 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 30/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9991\n",
      "Epoch 00030: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 31/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9990\n",
      "Epoch 00031: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0042 - acc: 0.9990 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 32/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9991\n",
      "Epoch 00032: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 33/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9991\n",
      "Epoch 00033: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 34/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0043 - acc: 0.9991\n",
      "Epoch 00034: acc improved from 0.99913 to 0.99914, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 35/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0045 - acc: 0.9991\n",
      "Epoch 00035: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 36/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0042 - acc: 0.9992\n",
      "Epoch 00036: acc improved from 0.99914 to 0.99919, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0042 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 37/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9991\n",
      "Epoch 00037: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 38/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
      "Epoch 00038: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0044 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 39/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9991\n",
      "Epoch 00039: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 40/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0043 - acc: 0.9991\n",
      "Epoch 00040: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 41/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0046 - acc: 0.9991\n",
      "Epoch 00041: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0045 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 42/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9991\n",
      "Epoch 00042: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 43/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
      "Epoch 00043: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0042 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 44/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0042 - acc: 0.9991\n",
      "Epoch 00044: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0041 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 45/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0042 - acc: 0.9991\n",
      "Epoch 00045: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 46/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9992\n",
      "Epoch 00046: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0043 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 47/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9992\n",
      "Epoch 00047: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0042 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 48/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0042 - acc: 0.9992\n",
      "Epoch 00048: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0042 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 49/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0049 - acc: 0.9991\n",
      "Epoch 00049: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0049 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 50/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9991\n",
      "Epoch 00050: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 51/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0043 - acc: 0.9992\n",
      "Epoch 00051: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0042 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 52/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0043 - acc: 0.9991\n",
      "Epoch 00052: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 53/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00053: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 54/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9991\n",
      "Epoch 00054: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0043 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 55/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9992\n",
      "Epoch 00055: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 56/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9991\n",
      "Epoch 00056: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0042 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 57/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0042 - acc: 0.9992\n",
      "Epoch 00057: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0043 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 58/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00058: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0039 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 59/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0045 - acc: 0.9990\n",
      "Epoch 00059: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0044 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 60/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0042 - acc: 0.9991\n",
      "Epoch 00060: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 61/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
      "Epoch 00061: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0040 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 62/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0042 - acc: 0.9992\n",
      "Epoch 00062: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0044 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 63/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0043 - acc: 0.9991\n",
      "Epoch 00063: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0042 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 64/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
      "Epoch 00064: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 65/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
      "Epoch 00065: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0041 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 66/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00066: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 67/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 00067: acc improved from 0.99919 to 0.99923, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0040 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 68/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9992\n",
      "Epoch 00068: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 69/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9992\n",
      "Epoch 00069: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 70/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9992\n",
      "Epoch 00070: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0042 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 71/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9991\n",
      "Epoch 00071: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0042 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 72/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9992\n",
      "Epoch 00072: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 73/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0043 - acc: 0.9991\n",
      "Epoch 00073: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 74/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
      "Epoch 00074: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0040 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9991\n",
      "Epoch 00075: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0039 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
      "Epoch 00076: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9991\n",
      "Epoch 00077: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9991\n",
      "Epoch 00078: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0040 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0043 - acc: 0.9991\n",
      "Epoch 00079: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9992\n",
      "Epoch 00080: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9991\n",
      "Epoch 00081: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0040 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
      "Epoch 00082: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0042 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9992\n",
      "Epoch 00083: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 84/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
      "Epoch 00084: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0041 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9992\n",
      "Epoch 00085: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9991\n",
      "Epoch 00086: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9991\n",
      "Epoch 00087: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9993\n",
      "Epoch 00088: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0040 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
      "Epoch 00089: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
      "Epoch 00090: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
      "Epoch 00091: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0039 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
      "Epoch 00092: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9992\n",
      "Epoch 00093: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9993\n",
      "Epoch 00094: acc improved from 0.99923 to 0.99926, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00095: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9992\n",
      "Epoch 00096: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9992\n",
      "Epoch 00097: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0042 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9992\n",
      "Epoch 00098: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00099: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
      "Epoch 00100: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9990\n",
      "Epoch 00101: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0040 - acc: 0.9990 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
      "Epoch 00102: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9991\n",
      "Epoch 00103: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00104: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9991\n",
      "Epoch 00105: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
      "Epoch 00106: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9992\n",
      "Epoch 00107: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 108/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9991\n",
      "Epoch 00108: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0040 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
      "Epoch 00109: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
      "Epoch 00110: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00111: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00112: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0043 - acc: 0.9991\n",
      "Epoch 00113: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0041 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00114: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0037 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 115/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00115: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9991\n",
      "Epoch 00116: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9991 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00117: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00118: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00119: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00120: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00121: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00122: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 123/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9992\n",
      "Epoch 00123: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00124: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00125: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9993\n",
      "Epoch 00126: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00127: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 00128: acc improved from 0.99926 to 0.99933, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00129: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00130: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00131: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9992\n",
      "Epoch 00132: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9993\n",
      "Epoch 00133: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      "78848/80028 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00134: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9993\n",
      "Epoch 00135: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00136: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00137: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00138: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00139: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0034 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00140: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00141: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00142: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00143: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9991\n",
      "Epoch 00144: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00145: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00146: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00147: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00148: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00149: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9991\n",
      "Epoch 00150: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00151: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00152: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00153: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00154: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00155: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 00156: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00157: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00158: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00159: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      "77824/80028 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00160: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00161: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00162: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00163: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00164: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00165: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9992\n",
      "Epoch 00166: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 167/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00167: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00168: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00169: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 170/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00170: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00171: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 172/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00172: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00173: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00174: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9992\n",
      "Epoch 00175: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00176: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 177/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9993\n",
      "Epoch 00177: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00178: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00179: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00180: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00181: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00182: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0035 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 183/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00183: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00184: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00185: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9993\n",
      "Epoch 00186: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00187: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00188: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9993\n",
      "Epoch 00189: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00190: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 00191: acc improved from 0.99933 to 0.99935, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00192: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00193: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00194: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00195: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00196: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00197: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00198: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00199: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00200: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00201: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 00202: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00203: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00204: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00205: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00206: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00207: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00208: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0037 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00209: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00210: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00211: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00212: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00213: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00214: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00215: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00216: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00217: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00218: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 00219: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00220: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00221: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00222: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 00223: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00224: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00225: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00226: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00227: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00228: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00229: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00230: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 231/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00231: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00232: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00233: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00234: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00235: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00236: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00237: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 238/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00238: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00239: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00240: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 00241: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 00242: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00243: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 244/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00244: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00245: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00246: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00247: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00248: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 00249: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00250: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00251: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 252/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00252: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00253: acc improved from 0.99935 to 0.99936, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00254: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9991\n",
      "Epoch 00255: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00256: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9993\n",
      "Epoch 00257: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 00258: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00259: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 260/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00260: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00261: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00262: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0032 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00263: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00264: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00265: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00266: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00267: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 00268: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00269: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00270: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00271: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00272: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00273: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00274: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00275: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00276: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00277: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00278: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00279: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 00280: acc improved from 0.99936 to 0.99939, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00281: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00282: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00283: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 00284: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00285: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00286: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00287: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00288: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00289: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00290: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00291: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0035 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00292: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 293/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00293: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00294: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00295: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00296: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00297: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00298: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00299: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 300/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00300: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00301: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00302: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00303: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00304: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00305: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00306: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 00307: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00308: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00309: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00310: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 311/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 00311: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 00312: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0033 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 313/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00313: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 314/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00314: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 315/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00315: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00316: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 317/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00317: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 318/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00318: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 319/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00319: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 320/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00320: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 321/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00321: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00322: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 323/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00323: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 324/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00324: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 325/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00325: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00326: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 327/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00327: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 328/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00328: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 329/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00329: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 330/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00330: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 331/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 00331: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 332/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00332: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 333/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00333: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 334/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 00334: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 335/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00335: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 336/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00336: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 337/500\n",
      "78848/80028 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00337: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 338/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00338: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 339/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00339: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 340/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00340: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00341: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 342/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00342: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 343/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00343: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 344/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00344: acc improved from 0.99939 to 0.99944, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 345/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00345: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 346/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00346: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00347: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 348/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00348: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 349/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00349: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 350/500\n",
      "77824/80028 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00350: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 351/500\n",
      "78848/80028 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00351: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 352/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00352: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 353/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 00353: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 354/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00354: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 355/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00355: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 356/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00356: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00357: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 358/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00358: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 359/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00359: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 360/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00360: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 361/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00361: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 362/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00362: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 363/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00363: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 364/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00364: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 365/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 00365: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 366/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00366: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 367/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00367: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00368: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 369/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00369: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00370: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 371/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00371: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00372: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 373/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00373: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 374/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00374: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 375/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00375: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 376/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00376: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00377: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 378/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00378: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 379/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00379: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 380/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00380: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 381/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 00381: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 382/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00382: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 383/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00383: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 384/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00384: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 385/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00385: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 386/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00386: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00387: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 388/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00388: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 389/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00389: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 390/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00390: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 391/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00391: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 392/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 00392: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 393/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00393: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 394/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00394: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 395/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00395: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 396/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00396: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 397/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00397: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 398/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00398: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 399/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9992\n",
      "Epoch 00399: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 400/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9992\n",
      "Epoch 00400: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 401/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00401: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 402/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00402: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 403/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00403: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9992\n",
      "Epoch 00404: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 405/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00405: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00406: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 407/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00407: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 408/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00408: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 409/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 00409: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 410/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00410: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 411/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00411: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00412: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 413/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00413: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00414: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 415/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00415: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00416: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00417: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 418/500\n",
      "78848/80028 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00418: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 419/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00419: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 420/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00420: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 421/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00421: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 00422: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 423/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00423: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 424/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00424: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 425/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00425: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 426/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00426: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 427/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00427: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 428/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00428: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 429/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00429: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 430/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00430: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 431/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 00431: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00432: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 433/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00433: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 434/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00434: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 435/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00435: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 436/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00436: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 437/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00437: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00438: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 439/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00439: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 440/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 00440: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 441/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 00441: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 442/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00442: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00443: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 444/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 00444: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 445/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00445: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 446/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00446: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 447/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00447: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 448/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00448: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 449/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00449: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 450/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00450: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 451/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00451: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 452/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00452: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 453/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00453: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00454: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 455/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00455: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 456/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00456: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 457/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 00457: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 458/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 00458: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 459/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00459: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 460/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00460: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 461/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 00461: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 462/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00462: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 463/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00463: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 464/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00464: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 465/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00465: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00466: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 467/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00467: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 468/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00468: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 469/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00469: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00470: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 471/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9993\n",
      "Epoch 00471: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0034 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 472/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00472: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 473/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00473: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 474/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00474: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 475/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00475: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 476/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00476: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 477/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9995\n",
      "Epoch 00477: acc improved from 0.99944 to 0.99946, saving model to results/phase_nn.hdf5\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9995 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 478/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00478: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00479: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 480/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00480: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 481/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00481: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 482/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00482: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 483/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00483: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 484/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00484: acc did not improve\n",
      "80028/80028 [==============================] - 1s 9us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 00485: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0025 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00486: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 487/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00487: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 00488: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 489/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 00489: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 490/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 00490: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 491/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00491: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 492/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 00492: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0025 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 493/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 00493: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 494/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00494: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 495/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 00495: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0025 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 496/500\n",
      "79872/80028 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00496: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 497/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00497: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 498/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 00498: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 499/500\n",
      "74752/80028 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00499: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 500/500\n",
      "77824/80028 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00500: acc did not improve\n",
      "80028/80028 [==============================] - 1s 10us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Max of acc: 0.9994626880591793, val_acc: 1.0\n",
      "Min of loss: 0.0023544726103541256, val_loss: 1.1920930376163597e-07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "def run_train_stacking():\n",
    "    stacking_file = \"results/phase_train_train.hdf5\"\n",
    "    h5f = h5py.File(stacking_file, \"r\")\n",
    "    dset_probability = h5f['/probability']\n",
    "    dset_y = h5f['/y']\n",
    "    x_train_stacking = dset_probability[:]\n",
    "    # x_train_stacking = np.reshape(x_train_stacking, (x_train_stacking.shape[0], 16))\n",
    "    y_train_stacking = dset_y[:]\n",
    "    print(x_train_stacking.shape)\n",
    "    print(x_train_stacking.shape)\n",
    "    model = CNN()\n",
    "    model.fit(x_train_stacking, y_train_stacking, layers=[64, 128, 256], verbose=1)\n",
    "    #model.fit(x_train_stacking, y_train_stacking)\n",
    "    model.save(\"results/phase_stacking.mdl\")\n",
    "\n",
    "run_train_stacking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13680, 5, 4)\n",
      "Shape: [(0, 2280), (1, 2280), (2, 2280), (3, 6840)]\n",
      "Accuracy: 69.67105263157895\n"
     ]
    }
   ],
   "source": [
    "def run_final_prediction():\n",
    "    stacking_file = \"results/phase_test.hdf5\"\n",
    "    h5f = h5py.File(stacking_file, \"r\")\n",
    "    dset_probability = h5f['/probability']\n",
    "    dset_y = h5f['/y']\n",
    "    x_test_stacking = dset_probability[:]\n",
    "    #x_test_stacking = np.reshape(x_test_stacking, (x_test_stacking.shape[0], 16))\n",
    "    y_test_stacking = dset_y[:]\n",
    "    print(x_test_stacking.shape)\n",
    "    print(\"Shape:\", sorted(Counter(y_test_stacking).items()))\n",
    "    model = CNN()\n",
    "    model.load(\"results/phase_stacking.mdl\")\n",
    "    model.predict(x_test_stacking, y_test_stacking)\n",
    "\n",
    "run_final_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6567"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [(0, 2280), (1, 323), (2, 226), (3, 3738)]\n",
    "2280+323+226+3738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
